{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOB8xWq+7PrWnHAdgU7MCTX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Hj0RywJRXQvr","executionInfo":{"status":"ok","timestamp":1755317391695,"user_tz":-330,"elapsed":11098,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# upload train dataset\n","from google.colab import files\n","\n","# Open file picker\n","uploaded = files.upload()\n","df = pd.read_csv(\"train_v9rqX0R.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"WudP9Lu1XRsK","executionInfo":{"status":"ok","timestamp":1755317404466,"user_tz":-330,"elapsed":12802,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"9a55b37b-c4aa-406a-c8b1-c8f3fdd5c1e4"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-ebb2111b-c6e7-4c93-b715-ce660232502a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ebb2111b-c6e7-4c93-b715-ce660232502a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving train_v9rqX0R.csv to train_v9rqX0R.csv\n"]}]},{"cell_type":"code","source":["# upload test dataset\n","from google.colab import files\n","\n","# Open file picker\n","uploaded = files.upload()\n","test = pd.read_csv(\"test_AbJTz2l.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"YK0-rUJFXYh5","executionInfo":{"status":"ok","timestamp":1755317415210,"user_tz":-330,"elapsed":10470,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"347a543c-7e5e-4d83-bbf7-3164005614b0"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-61092583-c47a-4843-b8d0-8407bcff9548\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-61092583-c47a-4843-b8d0-8407bcff9548\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving test_AbJTz2l.csv to test_AbJTz2l.csv\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zc_iYf1SW74P","executionInfo":{"status":"ok","timestamp":1755257022761,"user_tz":-330,"elapsed":30914,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"2df67570-625d-474a-b8ab-1e5b9ed7e3f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["NaNs in combined data after imputation: 5694\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 3}\n","Polynomial Regression with Lasso RMSE: 1044.03\n","Polynomial Regression with Lasso CV RMSE: 1099.08\n"]}],"source":["# Combine datasets for consistent imputation\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning and Preprocessing ---\n","# Regularize 'Item_Fat_Content'\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Handle missing 'Outlet_Size' values using mode by outlet type\n","size_map = combined_data.groupby(['Outlet_Type'])['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Handle missing 'Item_Weight' values by item identifier\n","item_weight_map = combined_data.groupby(['Item_Identifier'])['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Handle zero 'Item_Visibility' values by item identifier\n","# Create a series with mean visibility for each Item_Identifier where visibility is > 0\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","# Use the series to fill the zero values\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create 'Outlet_Age' feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","# Check for any remaining NaNs after imputation\n","print(f\"NaNs in combined data after imputation: {combined_data.isnull().sum().sum()}\")\n","\n","# Split data back into train and test\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Define features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train-Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Polynomial Regression with Lasso Tuning ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(\n","    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","    greater_is_better=False\n",")\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid.fit(X_train[numerical_features], y_train)\n","\n","best_poly_lasso = grid.best_estimator_\n","y_pred_poly = best_poly_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores = -grid.best_score_\n","\n","print(\"\\nBest params:\", grid.best_params_)\n","print(f\"Polynomial Regression with Lasso RMSE: {rmse_poly:.2f}\")\n","print(f\"Polynomial Regression with Lasso CV RMSE: {cv_scores:.2f}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"zuudC7lOf1is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x43m9iybf1f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4NZ4mSjhf1ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SAYpiZiYf1Zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n","from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","\n","# Combine datasets for consistent imputation\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning and Preprocessing ---\n","# Regularize 'Item_Fat_Content'\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Handle missing 'Outlet_Size' values using mode by outlet type\n","size_map = combined_data.groupby(['Outlet_Type'])['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Handle missing 'Item_Weight' values by item identifier\n","item_weight_map = combined_data.groupby(['Item_Identifier'])['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Handle zero 'Item_Visibility' values by item identifier\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create 'Outlet_Age' feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# --- Advanced Feature Engineering ---\n","# 1. Create a broader item type category\n","combined_data['Item_Type_Combined'] = combined_data['Item_Identifier'].apply(lambda x: x[:2])\n","combined_data['Item_Type_Combined'] = combined_data['Item_Type_Combined'].replace({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})\n","\n","# 2. Bin 'Item_MRP' into categories\n","combined_data['MRP_Category'] = pd.cut(combined_data['Item_MRP'], bins=4, labels=['Low', 'Medium', 'High', 'Very High'])\n","\n","# 3. Create interaction features\n","combined_data['Item_Outlet_Interactions'] = combined_data['Item_Type'].astype(str) + '_' + combined_data['Outlet_Type'].astype(str)\n","\n","# Label Encoding for categorical features, including the new ones\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type',\n","                    'Outlet_Type', 'Item_Type_Combined', 'MRP_Category', 'Item_Outlet_Interactions']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","# Check for any remaining NaNs after imputation\n","print(f\"NaNs in combined data after imputation: {combined_data.isnull().sum().sum()}\")\n","\n","# Split data back into train and test\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Define features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age', 'Item_Type_Combined',\n","            'MRP_Category', 'Item_Outlet_Interactions']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train-Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Polynomial Regression with Lasso Tuning ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(\n","    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","    greater_is_better=False\n",")\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=1000))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid.fit(X_train[numerical_features], y_train)\n","\n","best_poly_lasso = grid.best_estimator_\n","y_pred_poly = best_poly_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores = -grid.best_score_\n","\n","print(\"\\nBest params:\", grid.best_params_)\n","print(f\"Polynomial Regression with Lasso RMSE: {rmse_poly:.2f}\")\n","print(f\"Polynomial Regression with Lasso CV RMSE: {cv_scores:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzB20Xpjf1WE","executionInfo":{"status":"ok","timestamp":1755258350759,"user_tz":-330,"elapsed":99539,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"8162f122-14bc-467d-f0ec-649883b8b2e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NaNs in combined data after imputation: 5694\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 2}\n","Polynomial Regression with Lasso RMSE: 1059.26\n","Polynomial Regression with Lasso CV RMSE: 1107.41\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iMJ9qF14jnk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_VFTZ6xwjndh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OmHFOg_Ojna2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ofY0xZfWjnXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import Lasso, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","\n","# Combine datasets for consistent imputation\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning and Preprocessing ---\n","# Regularize 'Item_Fat_Content'\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Handle missing 'Outlet_Size' values using mode by outlet type\n","size_map = combined_data.groupby(['Outlet_Type'])['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Handle missing 'Item_Weight' values by item identifier\n","item_weight_map = combined_data.groupby(['Item_Identifier'])['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Handle zero 'Item_Visibility' values by item identifier\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create 'Outlet_Age' feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","# Split data back into train and test\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Define features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train-Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Train Base Models ---\n","print(\"Training base models...\")\n","\n","# Tuned Polynomial Lasso Model\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n","    ('lasso', Lasso(alpha=0.1, max_iter=10000))\n","])\n","poly_lasso.fit(X_train, y_train)\n","\n","# Tuned Random Forest Model\n","rf = RandomForestRegressor(n_estimators=200, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","# --- Stacking Ensemble ---\n","print(\"\\nCreating stacking ensemble...\")\n","\n","# Get predictions from base models on the validation set\n","numerical_features = X_val.select_dtypes(include=[np.number]).columns\n","val_pred_poly = poly_lasso.predict(X_val[numerical_features])\n","val_pred_rf = rf.predict(X_val)\n","\n","# Create a new dataset for the meta-model with base model predictions as features\n","stacking_features = pd.DataFrame({\n","    'poly_lasso_pred': val_pred_poly,\n","    'rf_pred': val_pred_rf\n","})\n","\n","# Train a simple Linear Regression meta-model on the new dataset\n","meta_model = LinearRegression()\n","meta_model.fit(stacking_features, y_val)\n","\n","# Generate final predictions on the validation set\n","final_predictions = meta_model.predict(stacking_features)\n","final_rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n","\n","print(f\"Final Stacking Ensemble RMSE: {final_rmse:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRZWxYR-jnTx","executionInfo":{"status":"ok","timestamp":1755258485107,"user_tz":-330,"elapsed":14251,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"6466e644-dc4d-42b7-b513-4445f1c53def"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training base models...\n","\n","Creating stacking ensemble...\n","Final Stacking Ensemble RMSE: 1051.40\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9mBzR1xgj7ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OyiOtHumj6v2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0rvAFPWgj6tD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eBaVx0A7j6pz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import Lasso, LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from xgboost import XGBRegressor\n","\n","# Combine datasets for consistent imputation\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning and Preprocessing ---\n","# Regularize 'Item_Fat_Content'\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Handle missing 'Outlet_Size' values using mode by outlet type\n","size_map = combined_data.groupby(['Outlet_Type'])['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Handle missing 'Item_Weight' values by item identifier\n","item_weight_map = combined_data.groupby(['Item_Identifier'])['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Handle zero 'Item_Visibility' values by item identifier\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create 'Outlet_Age' feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","# Split data back into train and test\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Define features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train-Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Train Base Models ---\n","print(\"Training base models...\")\n","\n","# Tuned Polynomial Lasso Model\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n","    ('lasso', Lasso(alpha=0.1, max_iter=10000))\n","])\n","poly_lasso.fit(X_train, y_train)\n","\n","# Tuned Random Forest Model\n","rf = RandomForestRegressor(n_estimators=200, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","# Tuned XGBoost Model\n","xgb = XGBRegressor(\n","    n_estimators=500,\n","    learning_rate=0.05,\n","    max_depth=6,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42\n",")\n","xgb.fit(X_train, y_train)\n","\n","# --- Stacking Ensemble ---\n","print(\"\\nCreating stacking ensemble...\")\n","\n","# Get predictions from base models on the validation set\n","numerical_features = X_val.select_dtypes(include=[np.number]).columns\n","val_pred_poly = poly_lasso.predict(X_val[numerical_features])\n","val_pred_rf = rf.predict(X_val)\n","val_pred_xgb = xgb.predict(X_val)\n","\n","# Create a new dataset for the meta-model with base model predictions as features\n","stacking_features = pd.DataFrame({\n","    'poly_lasso_pred': val_pred_poly,\n","    'rf_pred': val_pred_rf,\n","    'xgb_pred': val_pred_xgb\n","})\n","\n","# Train a simple Linear Regression meta-model on the new dataset\n","meta_model = LinearRegression()\n","meta_model.fit(stacking_features, y_val)\n","\n","# Generate final predictions on the validation set\n","final_predictions = meta_model.predict(stacking_features)\n","final_rmse = np.sqrt(mean_squared_error(y_val, final_predictions))\n","\n","print(f\"Final Stacking Ensemble RMSE: {final_rmse:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IO5j0RGzj6mx","executionInfo":{"status":"ok","timestamp":1755258564192,"user_tz":-330,"elapsed":15739,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"399ef7d2-e5de-41a5-8f82-79c5cb2e548c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training base models...\n","\n","Creating stacking ensemble...\n","Final Stacking Ensemble RMSE: 1051.32\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Pkujx8lxkn-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GfU0Nyhokn2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kj89z1U3knz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EDcAxhbhknvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn\n","\n","print(\"scikit-learn version:\", sklearn.__version__)\n","\n","# --- Combine train and test for consistent preprocessing ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning ---\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Fill missing Outlet_Size by Outlet_Type mode\n","size_map = combined_data.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Fill missing Item_Weight by Item_Identifier mean\n","item_weight_map = combined_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Replace zero Item_Visibility with mean per item\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Outlet_Age feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for ordinal features\n","le_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","for col in le_cols:\n","    combined_data[col] = LabelEncoder().fit_transform(combined_data[col])\n","\n","# --- Split data back ---\n","df_clean = combined_data[combined_data['source']=='train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source']=='test'].drop(['Item_Outlet_Sales','source'], axis=1)\n","\n","# --- Features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Preprocessing ---\n","numerical_features = ['Item_Weight', 'Item_MRP', 'Item_Visibility', 'Outlet_Age']\n","categorical_features = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","\n","# Fix for scikit-learn version differences\n","if int(sklearn.__version__.split('.')[1]) >= 2:  # version >=1.2\n","    ohe = OneHotEncoder(drop='first', sparse_output=False)\n","else:\n","    ohe = OneHotEncoder(drop='first', sparse=False)\n","\n","preprocessor = ColumnTransformer([\n","    ('num', StandardScaler(), numerical_features),\n","    ('cat', ohe, categorical_features)\n","])\n","\n","# --- RMSE scorer ---\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","                           greater_is_better=False)\n","\n","# --- Polynomial Lasso Pipeline ---\n","poly_lasso = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('poly', PolynomialFeatures(include_bias=False, interaction_only=True)),\n","    ('lasso', Lasso(max_iter=1000))\n","])\n","\n","# --- Grid Search for best degree and alpha ---\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [1e-5, 1e-4, 1e-3, 0.01, 0.05]\n","}\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","\n","# --- Log-transform target to reduce skew ---\n","y_train_log = np.log1p(y_train)\n","\n","grid.fit(X_train, y_train_log)\n","best_poly_lasso = grid.best_estimator_\n","\n","# --- Predictions on validation set ---\n","y_val_pred_log = best_poly_lasso.predict(X_val)\n","y_val_pred = np.expm1(y_val_pred_log)\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","cv_rmse = -grid.best_score_\n","\n","print(\"\\nBest Params:\", grid.best_params_)\n","print(f\"Validation RMSE: {rmse_val:.2f}\")\n","print(f\"Cross-Validated RMSE: {cv_rmse:.2f}\")\n","\n","# --- Predictions on test set ---\n","y_test_pred_log = best_poly_lasso.predict(X_test_final)\n","y_test_pred = np.expm1(y_test_pred_log)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"id":"8_JQs9IKknmW","executionInfo":{"status":"error","timestamp":1755259289160,"user_tz":-330,"elapsed":164197,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"d90a4f1a-93d7-4cd5-98d6-a00ca4532e22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["scikit-learn version: 1.6.1\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best Params: {'lasso__alpha': 0.001, 'poly__degree': 2}\n","Validation RMSE: 1115.06\n","Cross-Validated RMSE: 0.54\n"]},{"output_type":"error","ename":"ValueError","evalue":"Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3047299896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# --- Predictions on test set ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0my_test_pred_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_poly_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_pred_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_polynomial.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WzAdtqw-ndjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"neQRllG1ndZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KHInJ467ndLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4fGACR2IndI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tfT5LNVhndF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.impute import SimpleImputer\n","import sklearn\n","\n","print(\"scikit-learn version:\", sklearn.__version__)\n","\n","# --- Load your data ---\n","# df = pd.read_csv('train.csv')\n","# test = pd.read_csv('test.csv')\n","\n","# --- Combine train and test for consistent preprocessing ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning ---\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Fill missing Outlet_Size by Outlet_Type mode\n","size_map = combined_data.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Fill missing Item_Weight by Item_Identifier mean\n","item_weight_map = combined_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Replace zero Item_Visibility with mean per item\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Outlet_Age feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","le_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","for col in le_cols:\n","    combined_data[col] = LabelEncoder().fit_transform(combined_data[col])\n","\n","# --- Split data back ---\n","df_clean = combined_data[combined_data['source']=='train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source']=='test'].drop(['Item_Outlet_Sales','source'], axis=1)\n","\n","# --- Features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Preprocessing ---\n","numerical_features = ['Item_Weight', 'Item_MRP', 'Item_Visibility', 'Outlet_Age']\n","categorical_features = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","\n","# OneHotEncoder depending on sklearn version\n","if int(sklearn.__version__.split('.')[1]) >= 2:  # version >=1.2\n","    ohe = OneHotEncoder(drop='first', sparse_output=False)\n","else:\n","    ohe = OneHotEncoder(drop='first', sparse=False)\n","\n","# Imputers\n","num_imputer = SimpleImputer(strategy='mean')\n","cat_imputer = SimpleImputer(strategy='most_frequent')\n","\n","preprocessor = ColumnTransformer([\n","    ('num', Pipeline([('imputer', num_imputer), ('scaler', StandardScaler())]), numerical_features),\n","    ('cat', Pipeline([('imputer', cat_imputer), ('ohe', ohe)]), categorical_features)\n","])\n","\n","# --- RMSE scorer ---\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","                           greater_is_better=False)\n","\n","# --- Polynomial Lasso Pipeline ---\n","poly_lasso = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('poly', PolynomialFeatures(include_bias=False, interaction_only=True)),\n","    ('lasso', Lasso(max_iter=1000))\n","])\n","\n","# --- Grid Search ---\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [1e-5, 1e-4, 1e-3, 0.01, 0.05]\n","}\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","\n","# Log-transform target to reduce skew\n","y_train_log = np.log1p(y_train)\n","\n","grid.fit(X_train, y_train_log)\n","best_poly_lasso = grid.best_estimator_\n","\n","# --- Predictions on validation set ---\n","y_val_pred_log = best_poly_lasso.predict(X_val)\n","y_val_pred = np.expm1(y_val_pred_log)\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","cv_rmse = -grid.best_score_\n","\n","print(\"\\nBest Params:\", grid.best_params_)\n","print(f\"Validation RMSE: {rmse_val:.2f}\")\n","print(f\"Cross-Validated RMSE: {cv_rmse:.2f}\")\n","\n","# --- Predictions on test set ---\n","y_test_pred_log = best_poly_lasso.predict(X_test_final)\n","y_test_pred = np.expm1(y_test_pred_log)\n","\n","# --- Save predictions ---\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': y_test_pred\n","})\n","# submission.to_csv('submission_poly_lasso.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHJgySFgndC7","executionInfo":{"status":"ok","timestamp":1755259716306,"user_tz":-330,"elapsed":153409,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"74d4140b-058c-4a08-86e4-7651e0202160"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["scikit-learn version: 1.6.1\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best Params: {'lasso__alpha': 0.001, 'poly__degree': 2}\n","Validation RMSE: 1115.06\n","Cross-Validated RMSE: 0.54\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Taw_9BI4p4CI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RKlvj27Gp3_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"P3B5m2c-p38Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RdfALwL8p35L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Import libraries ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures, LabelEncoder\n","from sklearn.linear_model import Lasso\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","\n","# --- Load data ---\n","# df = pd.read_csv('train.csv')\n","# test = pd.read_csv('test.csv')\n","\n","# --- Combine datasets for consistent preprocessing ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning & Preprocessing ---\n","\n","# Regularize 'Item_Fat_Content'\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Fill missing 'Outlet_Size' by mode per Outlet_Type\n","size_map = combined_data.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Fill missing 'Item_Weight' by item mean\n","item_weight_map = combined_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Replace zero 'Item_Visibility' with mean per item\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create 'Outlet_Age'\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label encoding for categorical features\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","# --- Split back into train and test ---\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Define features & target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train-validation split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# Log-transform target\n","y_train_log = np.log1p(y_train)\n","\n","# --- Polynomial Regression with Lasso ---\n","\n","# RMSE scorer\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","                           greater_is_better=False)\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","grid.fit(X_train[numerical_features], y_train_log)\n","\n","best_poly_lasso = grid.best_estimator_\n","\n","# --- Predictions on validation set ---\n","y_val_pred_log = best_poly_lasso.predict(X_val[numerical_features])\n","y_val_pred = np.expm1(y_val_pred_log)  # inverse log-transform\n","y_val_pred = np.clip(y_val_pred, 0, None)  # avoid negative predictions\n","\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","\n","results_val = pd.DataFrame({'Actual': y_val, 'Predicted': y_val_pred})\n","print(results_val.head())\n","\n","print(\"\\nBest parameters:\", grid.best_params_)\n","print(f\"Validation RMSE: {rmse_val:.2f}\")\n","\n","# --- Predictions on test set ---\n","# Fill NaNs in test if any\n","X_test_final[numerical_features] = X_test_final[numerical_features].fillna(X_test_final[numerical_features].mean())\n","\n","y_test_pred_log = best_poly_lasso.predict(X_test_final[numerical_features])\n","y_test_pred = np.expm1(y_test_pred_log)\n","y_test_pred = np.clip(y_test_pred, 0, None)\n","\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': y_test_pred\n","})\n","\n","print(submission.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"foUR-pWpp4w1","executionInfo":{"status":"ok","timestamp":1755260150900,"user_tz":-330,"elapsed":17291,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"39b9a5ac-c189-46c0-bfd9-1cc8f2623efa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","         Actual    Predicted\n","7503  1743.0644  1084.408055\n","2957   356.8688   638.379893\n","7031   377.5086   617.024724\n","1084  5778.4782  4299.111010\n","856   2356.9320  2860.287037\n","\n","Best parameters: {'lasso__alpha': 0.01, 'poly__degree': 3}\n","Validation RMSE: 1065.97\n","  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n","0           FDW58            OUT049        1548.344534\n","1           FDW14            OUT017        1253.110118\n","2           NCN55            OUT010         508.108978\n","3           FDQ58            OUT017        2292.807338\n","4           FDY38            OUT027        5997.499073\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mveZHMHBq_9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MEagsTxQq_6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GCH4Osxrq_3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q-Cyw5Lgq_0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, LabelEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.impute import SimpleImputer\n","import sklearn\n","\n","print(\"scikit-learn version:\", sklearn.__version__)\n","\n","# --- Load your data ---\n","# df = pd.read_csv('train.csv')\n","# test = pd.read_csv('test.csv')\n","\n","# --- Combine train and test for consistent preprocessing ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning ---\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Fill missing Outlet_Size by Outlet_Type mode\n","size_map = combined_data.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Fill missing Item_Weight by Item_Identifier mean\n","item_weight_map = combined_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Replace zero Item_Visibility with mean per item\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","# Create Outlet_Age feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","le_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","for col in le_cols:\n","    combined_data[col] = LabelEncoder().fit_transform(combined_data[col])\n","\n","# --- Split back into train and test ---\n","df_clean = combined_data[combined_data['source']=='train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source']=='test'].drop(['Item_Outlet_Sales','source'], axis=1)\n","\n","# --- Features and target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Preprocessing ---\n","numerical_features = ['Item_Weight', 'Item_MRP', 'Item_Visibility', 'Outlet_Age']\n","categorical_features = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","\n","# OneHotEncoder depending on sklearn version\n","if int(sklearn.__version__.split('.')[1]) >= 2:  # version >=1.2\n","    ohe = OneHotEncoder(drop='first', sparse_output=False)\n","else:\n","    ohe = OneHotEncoder(drop='first', sparse=False)\n","\n","# Imputers\n","num_imputer = SimpleImputer(strategy='mean')\n","cat_imputer = SimpleImputer(strategy='most_frequent')\n","\n","preprocessor = ColumnTransformer([\n","    ('num', Pipeline([('imputer', num_imputer), ('scaler', StandardScaler())]), numerical_features),\n","    ('cat', Pipeline([('imputer', cat_imputer), ('ohe', ohe)]), categorical_features)\n","])\n","\n","# --- RMSE scorer ---\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","                           greater_is_better=False)\n","\n","# --- Polynomial Lasso Pipeline ---\n","poly_lasso = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('poly', PolynomialFeatures(include_bias=False, interaction_only=True)),\n","    ('lasso', Lasso(max_iter=1000))\n","])\n","\n","# --- Grid Search ---\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [1e-5, 1e-4, 1e-3, 0.01, 0.05]\n","}\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","\n","# Log-transform target to reduce skew\n","y_train_log = np.log1p(y_train)\n","\n","grid.fit(X_train, y_train_log)\n","best_poly_lasso = grid.best_estimator_\n","\n","# --- Predictions on validation set ---\n","y_val_pred_log = best_poly_lasso.predict(X_val)\n","y_val_pred = np.expm1(y_val_pred_log)  # Convert back to original scale\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","cv_rmse = -grid.best_score_\n","\n","print(\"\\nBest Params:\", grid.best_params_)\n","print(f\"Validation RMSE (original scale): {rmse_val:.2f}\")\n","print(f\"Cross-Validated RMSE (log scale): {cv_rmse:.2f}\")\n","\n","# --- Predictions on test set ---\n","y_test_pred_log = best_poly_lasso.predict(X_test_final)\n","y_test_pred = np.expm1(y_test_pred_log)  # Convert back to original scale\n","\n","# --- Save predictions ---\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': y_test_pred\n","})\n","# submission.to_csv('submission_poly_lasso.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLTf46CQq_w0","executionInfo":{"status":"ok","timestamp":1755260576103,"user_tz":-330,"elapsed":151451,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"bab039bd-9d59-4001-d661-825f7d3134d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["scikit-learn version: 1.6.1\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best Params: {'lasso__alpha': 0.001, 'poly__degree': 2}\n","Validation RMSE (original scale): 1115.06\n","Cross-Validated RMSE (log scale): 0.54\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_k4HDdmbsRfP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JOoVAHAOsRaD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ygp8QO8EsRXR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vsA5DhtssRT5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pVMLDve3sRQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.ensemble import RandomForestRegressor\n","import xgboost as xgb\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# --- Combine datasets for consistent imputation ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning ---\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","size_map = combined_data.groupby(['Outlet_Type'])['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","item_weight_map = combined_data.groupby(['Item_Identifier'])['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'].replace(0, np.nan).fillna(visibility_means)\n","\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    combined_data[col] = le.fit_transform(combined_data[col])\n","\n","print(f\"NaNs in combined data after imputation: {combined_data.isnull().sum().sum()}\")\n","\n","# --- Split back ---\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales','source'], axis=1)\n","\n","# --- Features & Target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- Common RMSE scorer ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n","\n","# --- Polynomial Regression + Lasso ---\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","param_grid_lasso = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","grid_lasso = GridSearchCV(poly_lasso, param_grid_lasso, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid_lasso.fit(X_train[numerical_features], y_train)\n","\n","y_pred_poly = grid_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores_poly = -grid_lasso.best_score_\n","\n","print(\"\\nPolynomial Regression + Lasso:\")\n","print(\"Best params:\", grid_lasso.best_params_)\n","print(f\"Validation RMSE: {rmse_poly:.2f}, CV RMSE: {cv_scores_poly:.2f}\")\n","\n","# --- Random Forest ---\n","rf = RandomForestRegressor(random_state=42)\n","param_grid_rf = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [None, 10, 20],\n","    'min_samples_split': [2, 5]\n","}\n","grid_rf = GridSearchCV(rf, param_grid_rf, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","grid_rf.fit(X_train, y_train)\n","y_pred_rf = grid_rf.predict(X_val)\n","rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n","cv_scores_rf = -grid_rf.best_score_\n","\n","print(\"\\nRandom Forest:\")\n","print(\"Best params:\", grid_rf.best_params_)\n","print(f\"Validation RMSE: {rmse_rf:.2f}, CV RMSE: {cv_scores_rf:.2f}\")\n","\n","# --- XGBoost ---\n","xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n","param_grid_xgb = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.05, 0.1, 0.2],\n","    'subsample': [0.7, 1]\n","}\n","grid_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","grid_xgb.fit(X_train, y_train)\n","y_pred_xgb = grid_xgb.predict(X_val)\n","rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n","cv_scores_xgb = -grid_xgb.best_score_\n","\n","print(\"\\nXGBoost:\")\n","print(\"Best params:\", grid_xgb.best_params_)\n","print(f\"Validation RMSE: {rmse_xgb:.2f}, CV RMSE: {cv_scores_xgb:.2f}\")\n","\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# --- XGBoost with RandomizedSearchCV ---\n","xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=500)\n","\n","param_dist = {\n","    'max_depth': [3, 4, 5, 6, 7, 8],\n","    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n","    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'min_child_weight': [1, 3, 5, 7],\n","    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n","    'reg_alpha': [0, 0.01, 0.1, 1, 10],\n","    'reg_lambda': [1, 1.5, 2, 3, 5]\n","}\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n","\n","random_search = RandomizedSearchCV(\n","    xgb_model,\n","    param_distributions=param_dist,\n","    n_iter=100,  # increase for more exhaustive search\n","    scoring=rmse_scorer,\n","    cv=kf,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","random_search.fit(X_train, y_train)\n","\n","best_xgb = random_search.best_estimator_\n","y_pred_xgb = best_xgb.predict(X_val)\n","rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n","\n","print(\"\\nTuned XGBoost:\")\n","print(\"Best params:\", random_search.best_params_)\n","print(f\"Validation RMSE: {rmse_xgb:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HIriFa4lsRK0","executionInfo":{"status":"ok","timestamp":1755261403823,"user_tz":-330,"elapsed":593890,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"f43b682d-5cba-4d9f-eb08-cc8aa754e611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NaNs in combined data after imputation: 5694\n","Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Polynomial Regression + Lasso:\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 3}\n","Validation RMSE: 1044.03, CV RMSE: 1099.08\n","Fitting 5 folds for each of 12 candidates, totalling 60 fits\n","\n","Random Forest:\n","Best params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n","Validation RMSE: 1056.43, CV RMSE: 1110.52\n","Fitting 5 folds for each of 36 candidates, totalling 180 fits\n","\n","XGBoost:\n","Best params: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1}\n","Validation RMSE: 1046.07, CV RMSE: 1090.36\n","Fitting 5 folds for each of 100 candidates, totalling 500 fits\n","\n","Tuned XGBoost:\n","Best params: {'subsample': 0.7, 'reg_lambda': 1.5, 'reg_alpha': 0.1, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.9}\n","Validation RMSE: 1042.94\n"]}]},{"cell_type":"markdown","source":["**GOOD RESULT**"],"metadata":{"id":"34zkgVMSDFDJ"}},{"cell_type":"markdown","source":["**GOOD RESULT**"],"metadata":{"id":"TUjWLJUZDKot"}},{"cell_type":"code","source":[],"metadata":{"id":"NJTRhD5Fvk7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VqKcQdaFvkuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"M8hNdYsNvkrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K2wib-qtvkoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from xgboost import XGBRegressor\n","import xgboost as xgb\n","\n","# --- Load Data ---\n","#df = pd.read_csv(\"train.csv\")\n","#test = pd.read_csv(\"test.csv\")\n","\n","# --- Combine datasets for consistent preprocessing ---\n","df['source'] = 'train'\n","test['source'] = 'test'\n","combined_data = pd.concat([df, test], ignore_index=True)\n","\n","# --- Data Cleaning ---\n","combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace({\n","    'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'\n","})\n","\n","# Fill missing Outlet_Size by Outlet_Type mode\n","size_map = combined_data.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode()[0]).to_dict()\n","combined_data['Outlet_Size'] = combined_data['Outlet_Size'].fillna(combined_data['Outlet_Type'].map(size_map))\n","\n","# Fill missing Item_Weight by Item_Identifier mean\n","item_weight_map = combined_data.groupby('Item_Identifier')['Item_Weight'].transform('mean')\n","combined_data['Item_Weight'] = combined_data['Item_Weight'].fillna(item_weight_map)\n","\n","# Replace zero Item_Visibility with mean per item\n","visibility_means = combined_data[combined_data['Item_Visibility'] > 0] \\\n","    .groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n","combined_data['Item_Visibility'] = combined_data['Item_Visibility'] \\\n","    .replace(0, np.nan).fillna(visibility_means)\n","\n","# Create Outlet_Age feature\n","combined_data['Outlet_Age'] = 2025 - combined_data['Outlet_Establishment_Year']\n","\n","# Label Encoding for categorical features\n","categorical_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n","for col in categorical_cols:\n","    combined_data[col] = LabelEncoder().fit_transform(combined_data[col])\n","\n","# --- Split back into train/test ---\n","df_clean = combined_data[combined_data['source'] == 'train'].drop('source', axis=1)\n","test_clean = combined_data[combined_data['source'] == 'test'].drop(['Item_Outlet_Sales', 'source'], axis=1)\n","\n","# --- Features & Target ---\n","features = ['Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","            'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","            'Outlet_Type', 'Outlet_Age']\n","X = df_clean[features]\n","y = df_clean['Item_Outlet_Sales']\n","X_test_final = test_clean[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- RMSE scorer ---\n","rmse_scorer = make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False)\n","\n","# --- Hyperparameter Tuning with RandomizedSearchCV ---\n","xgb_model = XGBRegressor(random_state=42, n_estimators=500)\n","\n","param_dist = {\n","    'max_depth': [3, 4, 5, 6, 7, 8],\n","    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n","    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'min_child_weight': [1, 3, 5, 7],\n","    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n","    'reg_alpha': [0, 0.01, 0.1, 1, 10],\n","    'reg_lambda': [1, 1.5, 2, 3, 5]\n","}\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","random_search = RandomizedSearchCV(\n","    xgb_model,\n","    param_distributions=param_dist,\n","    n_iter=50,\n","    scoring=rmse_scorer,\n","    cv=kf,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","random_search.fit(X_train, y_train)\n","best_params = random_search.best_params_\n","print(\"Best params from CV:\", best_params)\n","\n","# --- Train Final Model with DMatrix for Early Stopping ---\n","dtrain = xgb.DMatrix(X_train, label=y_train)\n","dval = xgb.DMatrix(X_val, label=y_val)\n","\n","params = best_params.copy()\n","params.update({\n","    'objective': 'reg:squarederror',\n","    'eval_metric': 'rmse'\n","})\n","\n","watchlist = [(dtrain, 'train'), (dval, 'eval')]\n","\n","bst = xgb.train(\n","    params,\n","    dtrain,\n","    num_boost_round=2000,\n","    evals=watchlist,\n","    early_stopping_rounds=50,\n","    verbose_eval=50\n",")\n","\n","# --- Validation Predictions ---\n","y_val_pred = bst.predict(dval)\n","rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n","print(f\"Validation RMSE (XGBoost): {rmse_val:.2f}\")\n","\n","# --- Test Predictions ---\n","dtest = xgb.DMatrix(X_test_final)\n","y_test_pred = bst.predict(dtest)\n","\n","# --- Save Submission ---\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': y_test_pred\n","})\n","submission.to_csv('submission_xgb_dmatrix.csv', index=False)\n","print(\"Submission file saved as submission_xgb_dmatrix.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Lv1jlMxvklK","executionInfo":{"status":"ok","timestamp":1755266446522,"user_tz":-330,"elapsed":139384,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"e77a8a99-3be5-4831-9eaa-6247356e7b0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 50 candidates, totalling 250 fits\n","Best params from CV: {'subsample': 0.7, 'reg_lambda': 2, 'reg_alpha': 10, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.2, 'colsample_bytree': 1.0}\n","[0]\ttrain-rmse:1710.10004\teval-rmse:1654.89882\n","[50]\ttrain-rmse:1348.39905\teval-rmse:1300.29511\n","[100]\ttrain-rmse:1183.75304\teval-rmse:1143.98030\n","[150]\ttrain-rmse:1111.92954\teval-rmse:1080.62382\n","[200]\ttrain-rmse:1079.92805\teval-rmse:1056.71540\n","[250]\ttrain-rmse:1063.37162\teval-rmse:1047.91884\n","[300]\ttrain-rmse:1050.83762\teval-rmse:1045.17648\n","[350]\ttrain-rmse:1039.37227\teval-rmse:1044.04975\n","[400]\ttrain-rmse:1029.55089\teval-rmse:1044.30767\n","[424]\ttrain-rmse:1024.90278\teval-rmse:1044.84804\n","Validation RMSE (XGBoost): 1044.88\n","Submission file saved as submission_xgb_dmatrix.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"aH1MpLolDSsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9L_SaXe3DSpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TcNHAUTHDSmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"829siEDsDSjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","import xgboost as xgb\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# --- Load data ---\n","# df = pd.read_csv(\"train.csv\")\n","# test = pd.read_csv(\"test.csv\")\n","\n","# 1) Standardize Item_Fat_Content\n","df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","test['Item_Fat_Content'] = test['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","\n","# 2) Handle missing values in Outlet_Size\n","size_map = {\n","    'Grocery Store': 'Small',\n","    'Supermarket Type2': 'Medium',\n","    'Supermarket Type3': 'Medium'\n","}\n","df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Type'].map(size_map))\n","test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Type'].map(size_map))\n","\n","size_map2 = {'Tier 2': 'Small'}\n","df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Location_Type'].map(size_map2))\n","test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Location_Type'].map(size_map2))\n","\n","# 3) Handle missing values in Item_Weight\n","df['Item_Weight'] = df['Item_Weight'].fillna(df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","test['Item_Weight'] = test['Item_Weight'].fillna(df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","\n","df['Item_Weight'] = df.groupby('Item_Type')['Item_Weight'].transform('mean')\n","test['Item_Weight'] = test.groupby('Item_Type')['Item_Weight'].transform('mean')\n","\n","# 4) Handle zeros in Item_Visibility\n","df['Item_Visibility'] = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","test['Item_Visibility'] = test.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","# Fill remaining zeros in test\n","test['Item_Visibility'] = test.groupby(['Item_Type', 'Item_Fat_Content'])['Item_Visibility'].transform('mean')\n","\n","# --- Additional Feature: Outlet_Age ---\n","df['Outlet_Age'] = 2025 - df['Outlet_Establishment_Year']\n","test['Outlet_Age'] = 2025 - test['Outlet_Establishment_Year']\n","\n","# --- Manual Encoding ---\n","# Item_Fat_Content  1,2\n","df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","test['Item_Fat_Content'] = test['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","\n","# Outlet_Size  1,2,3\n","df['Outlet_Size'] = df['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","test['Outlet_Size'] = test['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","\n","# Outlet_Location_Type  1,2,3\n","df['Outlet_Location_Type'] = df['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","test['Outlet_Location_Type'] = test['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","\n","# Outlet_Type  1,2,3,4\n","outlet_type_map = {\n","    'Grocery Store': 1,\n","    'Supermarket Type1': 2,\n","    'Supermarket Type2': 3,\n","    'Supermarket Type3': 4\n","}\n","df['Outlet_Type'] = df['Outlet_Type'].replace(outlet_type_map)\n","test['Outlet_Type'] = test['Outlet_Type'].replace(outlet_type_map)\n","\n","# --- Encode Item_Type as 1,2,3,... ---\n","df['Item_Type'], uniques = pd.factorize(df['Item_Type'])\n","test['Item_Type'] = pd.Categorical(test['Item_Type'], categories=uniques).codes\n","df['Item_Type'] = df['Item_Type'] + 1\n","test['Item_Type'] = test['Item_Type'] + 1\n","\n","# --- New Feature: Item_Sales_Frequency ---\n","# xgboost - 1041\n","#df['Item_Sales_Frequency'] = df['Outlet_Age'] * (df['Item_MRP'] - df['Item_Visibility'])/(df['Item_Weight'] + 1)\n","#test['Item_Sales_Frequency'] = test['Outlet_Age'] * (test['Item_MRP'] - test['Item_Visibility'])/(test['Item_Weight'] + 1)\n","\n","# polynomial regression - 1039\n","#item_popularity = df['Item_Identifier'].value_counts(normalize=True)  # normalized frequency\n","#df['Item_Popularity'] = df['Item_Identifier'].map(item_popularity)\n","#test['Item_Popularity'] = test['Item_Identifier'].map(item_popularity).fillna(0)  # unseen items  0\n","\n","# --- 2. Item_Sales_Frequency ---\n","#df['Item_Sales_Frequency'] = (\n","#    np.log1p(df['Outlet_Age']) * (df['Item_MRP'] / (df['Item_Weight'] + 1)) * df['Item_Popularity']\n","#)\n","\n","#test['Item_Sales_Frequency'] = (\n","#    np.log1p(test['Outlet_Age']) * (test['Item_MRP'] / (test['Item_Weight'] + 1)) * test['Item_Popularity']\n","#)\n","\n","### polynomial regression - 1038.26\n","#df['Item_Sales_Frequency'] = (\n","#    np.log1p(df['Outlet_Age']) *\n","#    ((df['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","#    (df['Item_Popularity'] + 0.01)  # smoothing\n","#)\n","\n","#test['Item_Sales_Frequency'] = (\n","#    np.log1p(test['Outlet_Age']) *\n","#    ((test['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","#    (test['Item_Popularity'] + 0.01)\n","#)\n","\n","\n","df['Item_Sales_Frequency'] = (\n","    np.log1p(df['Outlet_Age']) *\n","    ((df['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","    (df['Item_Popularity'] + 0.05) /  # slightly stronger smoothing\n","    (1 + np.sqrt(df['Item_Visibility']))  # gentler visibility penalty\n",")\n","\n","test['Item_Sales_Frequency'] = (\n","    np.log1p(test['Outlet_Age']) *\n","    ((test['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","    (test['Item_Popularity'] + 0.05) /\n","    (1 + np.sqrt(test['Item_Visibility']))\n",")\n","\n","\n","# --- Handle Inf / NaN in Item_Sales_Frequency ---\n","df['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","test['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","df['Item_Sales_Frequency'].fillna(df['Item_Sales_Frequency'].mean(), inplace=True)\n","test['Item_Sales_Frequency'].fillna(test['Item_Sales_Frequency'].mean(), inplace=True)\n","\n","\n","\n","# --- New Feature: Customer Outlet Preference ---\n","outlet_type_sales = df.groupby('Outlet_Type')['Item_Outlet_Sales'].sum()\n","outlet_type_percentage = outlet_type_sales / outlet_type_sales.sum()\n","\n","df['Outlet_Type_Percentage'] = df['Outlet_Type'].map(outlet_type_percentage)\n","test['Outlet_Type_Percentage'] = test['Outlet_Type'].map(outlet_type_percentage)\n","\n","# xgboost - 1041\n","#df['Customer_Outlet_Preference'] = (\n","#    df['Item_MRP'] * df['Outlet_Type_Percentage'] / (df['Item_Weight']+1)*(df['Item_Visibility']+1)\n","#)\n","#test['Customer_Outlet_Preference'] = (\n","#    test['Item_MRP'] * test['Outlet_Type_Percentage'] / (test['Item_Weight']+1)*(test['Item_Visibility']+1)\n","#)\n","\n","# polynomial regression - 1039\n","#median_mrp = df['Item_MRP'].median()\n","\n","#df['Customer_Outlet_Preference'] = (\n","#    ((df['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-df['Item_Visibility']) *\n","#    df['Outlet_Type_Percentage']\n","#)\n","\n","#test['Customer_Outlet_Preference'] = (\n","#    ((test['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-test['Item_Visibility']) *\n","#    test['Outlet_Type_Percentage']\n","#)\n","\n","### polynomial regression - 1038.26\n","#median_mrp = df['Item_MRP'].median()\n","\n","#df['Customer_Outlet_Preference'] = (\n","#    np.sqrt(df['Item_MRP'] / median_mrp) *\n","#    (1 / (1 + np.log1p(df['Item_Visibility']))) *   # softer visibility penalty\n","#    (df['Outlet_Type_Percentage'] * (1 / df['Outlet_Location_Type']))\n","#)\n","\n","#test['Customer_Outlet_Preference'] = (\n","#    np.sqrt(test['Item_MRP'] / median_mrp) *\n","#    (1 / (1 + np.log1p(test['Item_Visibility']))) *\n","#    (test['Outlet_Type_Percentage'] * (1 / test['Outlet_Location_Type']))\n","#)\n","\n","\n","median_mrp = df['Item_MRP'].median()\n","\n","df['Customer_Outlet_Preference'] = (\n","    np.sqrt(df['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.sqrt(df['Item_Visibility']))) *\n","    (df['Outlet_Type_Percentage']) *\n","    (1 + 0.05 * df['Outlet_Size'])  # small bonus, not overpowering\n",")\n","\n","test['Customer_Outlet_Preference'] = (\n","    np.sqrt(test['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.sqrt(test['Item_Visibility']))) *\n","    (test['Outlet_Type_Percentage']) *\n","    (1 + 0.05 * test['Outlet_Size'])\n",")\n","\n","\n","\n","# Normalize to 0-1 range\n","#df['Customer_Outlet_Preference'] = (\n","#    (df['Customer_Outlet_Preference'] - df['Customer_Outlet_Preference'].min()) /\n","#    (df['Customer_Outlet_Preference'].max() - df['Customer_Outlet_Preference'].min())\n","#)\n","#test['Customer_Outlet_Preference'] = (\n","#    (test['Customer_Outlet_Preference'] - test['Customer_Outlet_Preference'].min()) /\n","#    (test['Customer_Outlet_Preference'].max() - test['Customer_Outlet_Preference'].min())\n","#)\n","\n","# --- Features & Target ---\n","features = [\n","    'Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","    'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","    'Outlet_Type', 'Outlet_Age', 'Item_Sales_Frequency',\n","    'Customer_Outlet_Preference'\n","]\n","\n","X = df[features]\n","y = df['Item_Outlet_Sales']\n","X_test_final = test[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- RMSE scorer ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False)\n","\n","# --- Tuned XGBoost with RandomizedSearchCV ---\n","xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=500)\n","\n","param_dist = {\n","    'max_depth': [3, 4, 5, 6, 7, 8],\n","    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n","    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'min_child_weight': [1, 3, 5, 7],\n","    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n","    'reg_alpha': [0, 0.01, 0.1, 1, 10],\n","    'reg_lambda': [1, 1.5, 2, 3, 5]\n","}\n","\n","random_search = RandomizedSearchCV(\n","    xgb_model,\n","    param_distributions=param_dist,\n","    n_iter=100,\n","    scoring=rmse_scorer,\n","    cv=kf,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","random_search.fit(X_train, y_train)\n","\n","best_xgb = random_search.best_estimator_\n","y_pred_xgb = best_xgb.predict(X_val)\n","rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n","\n","print(\"\\nTuned XGBoost:\")\n","print(\"Best params:\", random_search.best_params_)\n","print(f\"Validation RMSE: {rmse_xgb:.2f}\")\n","\n","# --- Final predictions on test set ---\n","test_predictions = best_xgb.predict(X_test_final)\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': test_predictions\n","})\n","# submission.to_csv(\"submission.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdD3WGWxDSgs","executionInfo":{"status":"ok","timestamp":1755323155428,"user_tz":-330,"elapsed":254212,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"9727cc05-5590-4ee3-dc0e-2140c8185f57"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n","\n","Tuned XGBoost:\n","Best params: {'subsample': 0.7, 'reg_lambda': 1.5, 'reg_alpha': 0.1, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.9}\n","Validation RMSE: 1042.11\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# --- Polynomial Regression with Lasso Tuning ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(\n","    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","    greater_is_better=False\n",")\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid.fit(X_train[numerical_features], y_train)\n","\n","best_poly_lasso = grid.best_estimator_\n","y_pred_poly = best_poly_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores = -grid.best_score_\n","\n","print(\"\\nBest params:\", grid.best_params_)\n","print(f\"Polynomial Regression with Lasso RMSE: {rmse_poly:.2f}\")\n","print(f\"Polynomial Regression with Lasso CV RMSE: {cv_scores:.2f}\")"],"metadata":{"id":"d0YTXbMY2nYW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755323221745,"user_tz":-330,"elapsed":27355,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"bdf16ae6-2a6b-4594-faf7-734f0a7bc7f1"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 2}\n","Polynomial Regression with Lasso RMSE: 1040.30\n","Polynomial Regression with Lasso CV RMSE: 1089.18\n"]}]},{"cell_type":"code","source":["# RMSE scorer\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False)\n","\n","# --- Random Forest Regressor ---\n","rf = RandomForestRegressor(n_estimators=300, max_depth=12, random_state=42, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","\n","# Validation prediction\n","y_pred_rf = rf.predict(X_val)\n","rmse_rf = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n","rf_scores = cross_val_score(rf, X, y, cv=kf, scoring=rmse_scorer)\n","\n","print(f\"Random Forest Validation RMSE: {rmse_rf:.2f}\")\n","print(f\"Random Forest CV RMSE: {-rf_scores.mean():.2f}\")\n","\n","# Final predictions\n","test_predictions = rf.predict(X_test_final)\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': test_predictions\n","})\n","# submission.to_csv(\"rf_submission.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bFhLspbPfTW","executionInfo":{"status":"ok","timestamp":1755323290861,"user_tz":-330,"elapsed":45271,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"44213a72-0fd3-401e-a07c-49731766c672"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Validation RMSE: 1056.98\n","Random Forest CV RMSE: 1096.44\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QFJjkxZJPfGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0TYG813APe7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EgyVBDBIPe40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7h0CqCplbdeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XRZUMZ4OPe1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Imports ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","import xgboost as xgb\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# --- Load data ---\n","# df = pd.read_csv(\"train.csv\")\n","# test = pd.read_csv(\"test.csv\")\n","\n","# 1) Standardize Item_Fat_Content\n","df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","test['Item_Fat_Content'] = test['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","\n","# 2) Handle missing values in Outlet_Size\n","size_map = {\n","    'Grocery Store': 'Small',\n","    'Supermarket Type2': 'Medium',\n","    'Supermarket Type3': 'Medium'\n","}\n","df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Type'].map(size_map))\n","test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Type'].map(size_map))\n","\n","size_map2 = {'Tier 2': 'Small'}\n","df['Outlet_Size'] = df['Outlet_Size'].fillna(df['Outlet_Location_Type'].map(size_map2))\n","test['Outlet_Size'] = test['Outlet_Size'].fillna(test['Outlet_Location_Type'].map(size_map2))\n","\n","# 3) Handle missing values in Item_Weight\n","df['Item_Weight'] = df['Item_Weight'].fillna(df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","test['Item_Weight'] = test['Item_Weight'].fillna(df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","\n","df['Item_Weight'] = df.groupby('Item_Type')['Item_Weight'].transform('mean')\n","test['Item_Weight'] = test.groupby('Item_Type')['Item_Weight'].transform('mean')\n","\n","# 4) Handle zeros in Item_Visibility\n","df['Item_Visibility'] = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","test['Item_Visibility'] = test.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","# Fill remaining zeros in test\n","test['Item_Visibility'] = test.groupby(['Item_Type', 'Item_Fat_Content'])['Item_Visibility'].transform('mean')\n","\n","# --- Additional Feature: Outlet_Age ---\n","df['Outlet_Age'] = 2025 - df['Outlet_Establishment_Year']\n","test['Outlet_Age'] = 2025 - test['Outlet_Establishment_Year']\n","\n","# --- Manual Encoding ---\n","# Item_Fat_Content  1,2\n","df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","test['Item_Fat_Content'] = test['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","\n","# Outlet_Size  1,2,3\n","df['Outlet_Size'] = df['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","test['Outlet_Size'] = test['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","\n","# Outlet_Location_Type  1,2,3\n","df['Outlet_Location_Type'] = df['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","test['Outlet_Location_Type'] = test['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","\n","# Outlet_Type  1,2,3,4\n","outlet_type_map = {\n","    'Grocery Store': 1,\n","    'Supermarket Type1': 2,\n","    'Supermarket Type2': 3,\n","    'Supermarket Type3': 4\n","}\n","df['Outlet_Type'] = df['Outlet_Type'].replace(outlet_type_map)\n","test['Outlet_Type'] = test['Outlet_Type'].replace(outlet_type_map)\n","\n","# --- Encode Item_Type as 1,2,3,... ---\n","df['Item_Type'], uniques = pd.factorize(df['Item_Type'])\n","test['Item_Type'] = pd.Categorical(test['Item_Type'], categories=uniques).codes\n","df['Item_Type'] = df['Item_Type'] + 1\n","test['Item_Type'] = test['Item_Type'] + 1\n","\n","# --- New Feature: Item_Sales_Frequency ---\n","# xgboost - 1041\n","#df['Item_Sales_Frequency'] = df['Outlet_Age'] * (df['Item_MRP'] - df['Item_Visibility'])/(df['Item_Weight'] + 1)\n","#test['Item_Sales_Frequency'] = test['Outlet_Age'] * (test['Item_MRP'] - test['Item_Visibility'])/(test['Item_Weight'] + 1)\n","\n","# polynomial regression - 1039\n","#item_popularity = df['Item_Identifier'].value_counts(normalize=True)  # normalized frequency\n","#df['Item_Popularity'] = df['Item_Identifier'].map(item_popularity)\n","#test['Item_Popularity'] = test['Item_Identifier'].map(item_popularity).fillna(0)  # unseen items  0\n","\n","# --- 2. Item_Sales_Frequency ---\n","#df['Item_Sales_Frequency'] = (\n","#    np.log1p(df['Outlet_Age']) * (df['Item_MRP'] / (df['Item_Weight'] + 1)) * df['Item_Popularity']\n","#)\n","\n","#test['Item_Sales_Frequency'] = (\n","#    np.log1p(test['Outlet_Age']) * (test['Item_MRP'] / (test['Item_Weight'] + 1)) * test['Item_Popularity']\n","#)\n","\n","### polynomial regression - 1038.26\n","df['Item_Sales_Frequency'] = (\n","    np.log1p(df['Outlet_Age']) *\n","    ((df['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","    (df['Item_Popularity'] + 0.01)  # smoothing\n",")\n","\n","test['Item_Sales_Frequency'] = (\n","    np.log1p(test['Outlet_Age']) *\n","    ((test['Item_MRP'] - df['Item_MRP'].mean()) / (df['Item_MRP'].std() + 1)) *\n","    (test['Item_Popularity'] + 0.01)\n",")\n","\n","\n","\n","\n","# --- Handle Inf / NaN in Item_Sales_Frequency ---\n","df['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","test['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","df['Item_Sales_Frequency'].fillna(df['Item_Sales_Frequency'].mean(), inplace=True)\n","test['Item_Sales_Frequency'].fillna(test['Item_Sales_Frequency'].mean(), inplace=True)\n","\n","\n","\n","# --- New Feature: Customer Outlet Preference ---\n","outlet_type_sales = df.groupby('Outlet_Type')['Item_Outlet_Sales'].sum()\n","outlet_type_percentage = outlet_type_sales / outlet_type_sales.sum()\n","\n","df['Outlet_Type_Percentage'] = df['Outlet_Type'].map(outlet_type_percentage)\n","test['Outlet_Type_Percentage'] = test['Outlet_Type'].map(outlet_type_percentage)\n","\n","# xgboost - 1041\n","#df['Customer_Outlet_Preference'] = (\n","#    df['Item_MRP'] * df['Outlet_Type_Percentage'] / (df['Item_Weight']+1)*(df['Item_Visibility']+1)\n","#)\n","#test['Customer_Outlet_Preference'] = (\n","#    test['Item_MRP'] * test['Outlet_Type_Percentage'] / (test['Item_Weight']+1)*(test['Item_Visibility']+1)\n","#)\n","\n","# polynomial regression - 1039\n","#median_mrp = df['Item_MRP'].median()\n","\n","#df['Customer_Outlet_Preference'] = (\n","#    ((df['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-df['Item_Visibility']) *\n","#    df['Outlet_Type_Percentage']\n","#)\n","\n","#test['Customer_Outlet_Preference'] = (\n","#    ((test['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-test['Item_Visibility']) *\n","#    test['Outlet_Type_Percentage']\n","#)\n","\n","### polynomial regression - 1038.26\n","median_mrp = df['Item_MRP'].median()\n","\n","df['Customer_Outlet_Preference'] = (\n","    np.sqrt(df['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.log1p(df['Item_Visibility']))) *   # softer visibility penalty\n","    (df['Outlet_Type_Percentage'] * (1 / df['Outlet_Location_Type']))\n",")\n","\n","test['Customer_Outlet_Preference'] = (\n","    np.sqrt(test['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.log1p(test['Item_Visibility']))) *\n","    (test['Outlet_Type_Percentage'] * (1 / test['Outlet_Location_Type']))\n",")\n","\n","\n","\n","# --- New Features ---\n","# polynomial regression - 1038.14\n","# 1. Price per weight\n","df['Price_per_Weight'] = df['Item_MRP'] / (df['Item_Weight'] + 1)\n","test['Price_per_Weight'] = test['Item_MRP'] / (test['Item_Weight'] + 1)\n","\n","# 2. Visibility  Price interaction\n","df['Visibility_Price_Interaction'] = df['Item_Visibility'] * df['Item_MRP']\n","test['Visibility_Price_Interaction'] = test['Item_Visibility'] * test['Item_MRP']\n","\n","# 3. Outlet Age  Outlet Type\n","df['Outlet_Age_Type'] = df['Outlet_Age'] * df['Outlet_Type']\n","test['Outlet_Age_Type'] = test['Outlet_Age'] * test['Outlet_Type']\n","\n","# 4. Fat  Type interaction\n","df['Fat_Type_Interaction'] = df['Item_Fat_Content'] * df['Item_Type']\n","test['Fat_Type_Interaction'] = test['Item_Fat_Content'] * test['Item_Type']\n","\n","# 5. Outlet diversity\n","outlet_diversity = df.groupby('Outlet_Identifier')['Item_Identifier'].nunique()\n","df['Outlet_Diversity'] = df['Outlet_Identifier'].map(outlet_diversity)\n","test['Outlet_Diversity'] = test['Outlet_Identifier'].map(outlet_diversity).fillna(outlet_diversity.mean())\n","\n","# 6. Log features\n","df['Log_MRP'] = np.log1p(df['Item_MRP'])\n","test['Log_MRP'] = np.log1p(test['Item_MRP'])\n","\n","df['Log_Visibility'] = np.log1p(df['Item_Visibility'])\n","test['Log_Visibility'] = np.log1p(test['Item_Visibility'])\n","\n","\n","\n","# Normalize to 0-1 range\n","#df['Customer_Outlet_Preference'] = (\n","#    (df['Customer_Outlet_Preference'] - df['Customer_Outlet_Preference'].min()) /\n","#    (df['Customer_Outlet_Preference'].max() - df['Customer_Outlet_Preference'].min())\n","#)\n","#test['Customer_Outlet_Preference'] = (\n","#    (test['Customer_Outlet_Preference'] - test['Customer_Outlet_Preference'].min()) /\n","#    (test['Customer_Outlet_Preference'].max() - test['Customer_Outlet_Preference'].min())\n","#)\n","\n","# --- Features & Target ---\n","#features = [\n","#    'Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","#    'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","#    'Outlet_Type', 'Outlet_Age', 'Item_Sales_Frequency',\n","#    'Customer_Outlet_Preference'\n","#]\n","\n","\n","features = [\n","    'Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","    'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","    'Outlet_Type', 'Outlet_Age',\n","    'Item_Sales_Frequency', 'Customer_Outlet_Preference',\n","    'Price_per_Weight', 'Visibility_Price_Interaction',\n","    'Outlet_Age_Type', 'Fat_Type_Interaction',\n","    'Outlet_Diversity', 'Log_MRP', 'Log_Visibility'\n","]\n","\n","\n","\n","X = df[features]\n","y = df['Item_Outlet_Sales']\n","X_test_final = test[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n","\n","# --- RMSE scorer ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(lambda yt, yp: np.sqrt(mean_squared_error(yt, yp)), greater_is_better=False)\n","\n","# --- Tuned XGBoost with RandomizedSearchCV ---\n","xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=500)\n","\n","param_dist = {\n","    'max_depth': [3, 4, 5, 6, 7, 8],\n","    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n","    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n","    'min_child_weight': [1, 3, 5, 7],\n","    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n","    'reg_alpha': [0, 0.01, 0.1, 1, 10],\n","    'reg_lambda': [1, 1.5, 2, 3, 5]\n","}\n","\n","random_search = RandomizedSearchCV(\n","    xgb_model,\n","    param_distributions=param_dist,\n","    n_iter=100,\n","    scoring=rmse_scorer,\n","    cv=kf,\n","    verbose=1,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","random_search.fit(X_train, y_train)\n","\n","best_xgb = random_search.best_estimator_\n","y_pred_xgb = best_xgb.predict(X_val)\n","rmse_xgb = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n","\n","print(\"\\nTuned XGBoost:\")\n","print(\"Best params:\", random_search.best_params_)\n","print(f\"Validation RMSE: {rmse_xgb:.2f}\")\n","\n","# --- Final predictions on test set ---\n","test_predictions = best_xgb.predict(X_test_final)\n","submission = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': test_predictions\n","})\n","# submission.to_csv(\"submission.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3mQyL2-Pevy","executionInfo":{"status":"ok","timestamp":1755324130182,"user_tz":-330,"elapsed":442526,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"2eb1fcc3-b409-44c9-c9a7-7a49fc504bfa"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 100 candidates, totalling 500 fits\n","\n","Tuned XGBoost:\n","Best params: {'subsample': 1.0, 'reg_lambda': 2, 'reg_alpha': 0, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 1.0}\n","Validation RMSE: 1044.13\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# --- Polynomial Regression with Lasso Tuning ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(\n","    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","    greater_is_better=False\n",")\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid.fit(X_train[numerical_features], y_train)\n","\n","best_poly_lasso = grid.best_estimator_\n","y_pred_poly = best_poly_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores = -grid.best_score_\n","\n","print(\"\\nBest params:\", grid.best_params_)\n","print(f\"Polynomial Regression with Lasso RMSE: {rmse_poly:.2f}\")\n","print(f\"Polynomial Regression with Lasso CV RMSE: {cv_scores:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etka5nr8clpy","executionInfo":{"status":"ok","timestamp":1755325202621,"user_tz":-330,"elapsed":105226,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"9b23cb55-2c28-4f2b-aa41-c6881fbfb8b5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 2}\n","Polynomial Regression with Lasso RMSE: 1038.14\n","Polynomial Regression with Lasso CV RMSE: 1091.63\n"]}]},{"cell_type":"code","source":["# --- Final predictions on test set (Poly + Lasso) ---\n","test_predictions_poly = best_poly_lasso.predict(X_test_final[numerical_features])\n","\n","submission_poly = pd.DataFrame({\n","    'Item_Identifier': test['Item_Identifier'],\n","    'Outlet_Identifier': test['Outlet_Identifier'],\n","    'Item_Outlet_Sales': test_predictions_poly\n","})\n","submission_poly.to_csv(\"submission_poly.csv\", index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"AX2A5r3Ee-SB","executionInfo":{"status":"error","timestamp":1755325213446,"user_tz":-330,"elapsed":41,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"a857b632-9ba9-4aaa-f3c5-47ec4e1d869c"},"execution_count":30,"outputs":[{"output_type":"error","ename":"NotFittedError","evalue":"Pipeline is not fitted yet.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotFittedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \"\"\"\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4023335323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Final predictions on test set (Poly + Lasso) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_predictions_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoly_lasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m submission_poly = pd.DataFrame({\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'Item_Identifier'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Item_Identifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'Outlet_Identifier'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Outlet_Identifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \"\"\"\n\u001b[1;32m    781\u001b[0m         \u001b[0;31m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_raise_or_warn_if_not_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_raise_or_warn_if_not_fitted\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotFittedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline is not fitted yet.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# we only get here if the above didn't raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFittedError\u001b[0m: Pipeline is not fitted yet."]}]}]}