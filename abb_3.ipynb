{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPzVVdUapX5Nhj+MZsE9Nun"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, make_scorer\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Hj0RywJRXQvr","executionInfo":{"status":"ok","timestamp":1755326703902,"user_tz":-330,"elapsed":14138,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# upload train dataset\n","from google.colab import files\n","\n","# Open file picker\n","uploaded = files.upload()\n","train_df = pd.read_csv(\"train_v9rqX0R.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"WudP9Lu1XRsK","executionInfo":{"status":"ok","timestamp":1755326718050,"user_tz":-330,"elapsed":14152,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"72c08e82-dd45-4790-88b1-61af8248595c"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d199de65-b82d-4943-9a7a-d8dc6d31a5d2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d199de65-b82d-4943-9a7a-d8dc6d31a5d2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving train_v9rqX0R.csv to train_v9rqX0R.csv\n"]}]},{"cell_type":"code","source":["# upload test dataset\n","from google.colab import files\n","\n","# Open file picker\n","uploaded = files.upload()\n","valid_df = pd.read_csv(\"test_AbJTz2l.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"YK0-rUJFXYh5","executionInfo":{"status":"ok","timestamp":1755326727624,"user_tz":-330,"elapsed":9547,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"fe60ab2a-43f1-4979-b912-5f3c83493730"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-161d2f4b-6a82-4a9d-813d-6a540459b492\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-161d2f4b-6a82-4a9d-813d-6a540459b492\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving test_AbJTz2l.csv to test_AbJTz2l.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XRZUMZ4OPe1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Load data ---\n","# df = pd.read_csv(\"train.csv\")\n","# test = pd.read_csv(\"test.csv\")\n","\n","# 1) Standardize Item_Fat_Content\n","train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","valid_df['Item_Fat_Content'] = valid_df['Item_Fat_Content'].replace({\n","    'low fat': 'Low Fat',\n","    'LF': 'Low Fat',\n","    'reg': 'Regular'\n","})\n","\n","# 2) Handle missing values in Outlet_Size\n","size_map = {\n","    'Grocery Store': 'Small',\n","    'Supermarket Type2': 'Medium',\n","    'Supermarket Type3': 'Medium'\n","}\n","train_df['Outlet_Size'] = train_df['Outlet_Size'].fillna(train_df['Outlet_Type'].map(size_map))\n","valid_df['Outlet_Size'] = valid_df['Outlet_Size'].fillna(valid_df['Outlet_Type'].map(size_map))\n","\n","size_map2 = {'Tier 2': 'Small'}\n","train_df['Outlet_Size'] = train_df['Outlet_Size'].fillna(train_df['Outlet_Location_Type'].map(size_map2))\n","valid_df['Outlet_Size'] = valid_df['Outlet_Size'].fillna(valid_df['Outlet_Location_Type'].map(size_map2))\n","\n","# 3) Handle missing values in Item_Weight\n","train_df['Item_Weight'] = train_df['Item_Weight'].fillna(train_df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","valid_df['Item_Weight'] = valid_df['Item_Weight'].fillna(valid_df.groupby(['Item_Identifier'])['Item_Weight'].transform('mean'))\n","\n","#train_df['Item_Weight'] = train_df.groupby('Item_Type')['Item_Weight'].transform('mean')\n","#valid_df['Item_Weight'] = valid_df.groupby('Item_Type')['Item_Weight'].transform('mean')\n","\n","train_df['Item_Weight'] = train_df['Item_Weight'].fillna(train_df.groupby('Item_Type')['Item_Weight'].transform('mean'))\n","valid_df['Item_Weight'] = valid_df['Item_Weight'].fillna(valid_df.groupby('Item_Type')['Item_Weight'].transform('mean'))\n","\n","# 4) Handle zeros in Item_Visibility\n","train_df['Item_Visibility'] = train_df.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","valid_df['Item_Visibility'] = valid_df.groupby('Item_Identifier')['Item_Visibility'].transform(\n","    lambda x: x.replace(0, x.mean())\n",")\n","# Fill remaining zeros in test\n","valid_df['Item_Visibility'] = valid_df.groupby(['Item_Type', 'Item_Fat_Content'])['Item_Visibility'].transform('mean')\n","\n","# --- Additional Feature: Outlet_Age ---\n","train_df['Outlet_Age'] = 2025 - train_df['Outlet_Establishment_Year']\n","valid_df['Outlet_Age'] = 2025 - valid_df['Outlet_Establishment_Year']\n","\n","# --- Manual Encoding ---\n","# Item_Fat_Content → 1,2\n","train_df['Item_Fat_Content'] = train_df['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","valid_df['Item_Fat_Content'] = valid_df['Item_Fat_Content'].replace({'Low Fat': 1, 'Regular': 2})\n","\n","# Outlet_Size → 1,2,3\n","train_df['Outlet_Size'] = train_df['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","valid_df['Outlet_Size'] = valid_df['Outlet_Size'].replace({'Small': 1, 'Medium': 2, 'High': 3})\n","\n","# Outlet_Location_Type → 1,2,3\n","train_df['Outlet_Location_Type'] = train_df['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","valid_df['Outlet_Location_Type'] = valid_df['Outlet_Location_Type'].replace({'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3})\n","\n","# Outlet_Type → 1,2,3,4\n","outlet_type_map = {\n","    'Grocery Store': 1,\n","    'Supermarket Type1': 2,\n","    'Supermarket Type2': 3,\n","    'Supermarket Type3': 4\n","}\n","train_df['Outlet_Type'] = train_df['Outlet_Type'].replace(outlet_type_map)\n","valid_df['Outlet_Type'] = valid_df['Outlet_Type'].replace(outlet_type_map)\n","\n","# --- Encode Item_Type as 1,2,3,... ---\n","train_df['Item_Type'], uniques = pd.factorize(train_df['Item_Type'])\n","valid_df['Item_Type'] = pd.Categorical(valid_df['Item_Type'], categories=uniques).codes\n","\n","train_df['Item_Type'] = train_df['Item_Type'] + 1\n","valid_df['Item_Type'] = valid_df['Item_Type'] + 1\n","\n","# --- New Feature: Item_Sales_Frequency ---\n","# xgboost - 1041\n","#train_df['Item_Sales_Frequency'] = train_df['Outlet_Age'] * (train_df['Item_MRP'] - train_df['Item_Visibility'])/(train_df['Item_Weight'] + 1)\n","#valid_df['Item_Sales_Frequency'] = valid_df['Outlet_Age'] * (valid_df['Item_MRP'] - valid_df['Item_Visibility'])/(valid_df['Item_Weight'] + 1)\n","\n","# polynomial regression - 1039\n","#item_popularity = train_df['Item_Identifier'].value_counts(normalize=True)  # normalized frequency\n","#train_df['Item_Popularity'] = train_df['Item_Identifier'].map(item_popularity)\n","#valid_df['Item_Popularity'] = valid_df['Item_Identifier'].map(item_popularity).fillna(0)  # unseen items → 0\n","\n","# --- 2. Item_Sales_Frequency ---\n","#train_df['Item_Sales_Frequency'] = (\n","#    np.log1p(train_df['Outlet_Age']) * (train_df['Item_MRP'] / (train_df['Item_Weight'] + 1)) * train_df['Item_Popularity']\n","#)\n","\n","#valid_df['Item_Sales_Frequency'] = (\n","#    np.log1p(valid_df['Outlet_Age']) * (valid_df['Item_MRP'] / (valid_df['Item_Weight'] + 1)) * valid_df['Item_Popularity']\n","#)\n","\n","### polynomial regression - 1038.26\n","item_popularity = train_df['Item_Identifier'].value_counts(normalize=True)  # normalized frequency\n","\n","train_df['Item_Popularity'] = train_df['Item_Identifier'].map(item_popularity)\n","valid_df['Item_Popularity'] = valid_df['Item_Identifier'].map(item_popularity).fillna(0)  # unseen items → 0\n","\n","train_df['Item_Sales_Frequency'] = (\n","    np.log1p(train_df['Outlet_Age']) *\n","    ((train_df['Item_MRP'] - train_df['Item_MRP'].mean()) / (train_df['Item_MRP'].std() + 1)) *\n","    (train_df['Item_Popularity'] + 0.01)  # smoothing\n",")\n","\n","valid_df['Item_Sales_Frequency'] = (\n","    np.log1p(valid_df['Outlet_Age']) *\n","    ((valid_df['Item_MRP'] - valid_df['Item_MRP'].mean()) / (valid_df['Item_MRP'].std() + 1)) *\n","    (valid_df['Item_Popularity'] + 0.01)\n",")\n","\n","\n","\n","\n","# --- Handle Inf / NaN in Item_Sales_Frequency ---\n","train_df['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","valid_df['Item_Sales_Frequency'].replace([np.inf, -np.inf], np.nan, inplace=True)\n","train_df['Item_Sales_Frequency'].fillna(train_df['Item_Sales_Frequency'].mean(), inplace=True)\n","valid_df['Item_Sales_Frequency'].fillna(valid_df['Item_Sales_Frequency'].mean(), inplace=True)\n","\n","\n","\n","# --- New Feature: Customer Outlet Preference ---\n","outlet_type_sales = train_df.groupby('Outlet_Type')['Item_Outlet_Sales'].sum()\n","outlet_type_percentage = outlet_type_sales / outlet_type_sales.sum()\n","\n","train_df['Outlet_Type_Percentage'] = train_df['Outlet_Type'].map(outlet_type_percentage)\n","valid_df['Outlet_Type_Percentage'] = valid_df['Outlet_Type'].map(outlet_type_percentage)\n","\n","# xgboost - 1041\n","#train_df['Customer_Outlet_Preference'] = (\n","#    train_df['Item_MRP'] * train_df['Outlet_Type_Percentage'] / (train_df['Item_Weight']+1)*(train_df['Item_Visibility']+1)\n","#)\n","#valid_df['Customer_Outlet_Preference'] = (\n","#    valid_df['Item_MRP'] * valid_df['Outlet_Type_Percentage'] / (valid_df['Item_Weight']+1)*(valid_df['Item_Visibility']+1)\n","#)\n","\n","# polynomial regression - 1039\n","#median_mrp = df['Item_MRP'].median()\n","\n","#train_df['Customer_Outlet_Preference'] = (\n","#    ((train_df['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-train_df['Item_Visibility']) *\n","#    train_df['Outlet_Type_Percentage']\n","#)\n","\n","#valid_df['Customer_Outlet_Preference'] = (\n","#    ((valid_df['Item_MRP'] / median_mrp) ** 0.5) *\n","#    np.exp(-valid_df['Item_Visibility']) *\n","#    valid_df['Outlet_Type_Percentage']\n","#)\n","\n","### polynomial regression - 1038.26\n","median_mrp = train_df['Item_MRP'].median()\n","\n","train_df['Customer_Outlet_Preference'] = (\n","    np.sqrt(train_df['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.log1p(train_df['Item_Visibility']))) *   # softer visibility penalty\n","    (train_df['Outlet_Type_Percentage'] * (1 / train_df['Outlet_Location_Type']))\n",")\n","\n","valid_df['Customer_Outlet_Preference'] = (\n","    np.sqrt(valid_df['Item_MRP'] / median_mrp) *\n","    (1 / (1 + np.log1p(valid_df['Item_Visibility']))) *\n","    (valid_df['Outlet_Type_Percentage'] * (1 / valid_df['Outlet_Location_Type']))\n",")\n","\n","\n","\n","# --- New Features ---\n","# polynomial regression + above equations + these equations - 1038.14\n","# 1. Price per weight\n","train_df['Price_per_Weight'] = train_df['Item_MRP'] / (train_df['Item_Weight'] + 1)\n","valid_df['Price_per_Weight'] = valid_df['Item_MRP'] / (valid_df['Item_Weight'] + 1)\n","\n","# 2. Visibility × Price interaction\n","train_df['Visibility_Price_Interaction'] = train_df['Item_Visibility'] * train_df['Item_MRP']\n","valid_df['Visibility_Price_Interaction'] = valid_df['Item_Visibility'] * valid_df['Item_MRP']\n","\n","# 3. Outlet Age × Outlet Type\n","train_df['Outlet_Age_Type'] = train_df['Outlet_Age'] * train_df['Outlet_Type']\n","valid_df['Outlet_Age_Type'] = valid_df['Outlet_Age'] * valid_df['Outlet_Type']\n","\n","# 4. Fat × Type interaction\n","train_df['Fat_Type_Interaction'] = train_df['Item_Fat_Content'] * train_df['Item_Type']\n","valid_df['Fat_Type_Interaction'] = valid_df['Item_Fat_Content'] * valid_df['Item_Type']\n","\n","# 5. Outlet diversity\n","outlet_diversity = train_df.groupby('Outlet_Identifier')['Item_Identifier'].nunique()\n","train_df['Outlet_Diversity'] = train_df['Outlet_Identifier'].map(outlet_diversity)\n","valid_df['Outlet_Diversity'] = valid_df['Outlet_Identifier'].map(outlet_diversity)\n","valid_df['Outlet_Diversity'].fillna(outlet_diversity.mean(), inplace=True)\n","\n","# 6. Log features\n","train_df['Log_MRP'] = np.log1p(train_df['Item_MRP'])\n","valid_df['Log_MRP'] = np.log1p(valid_df['Item_MRP'])\n","\n","train_df['Log_Visibility'] = np.log1p(train_df['Item_Visibility'])\n","valid_df['Log_Visibility'] = np.log1p(valid_df['Item_Visibility'])\n","\n","\n","\n","# Normalize to 0-1 range\n","#train_df['Customer_Outlet_Preference'] = (\n","#    (train_df['Customer_Outlet_Preference'] - train_df['Customer_Outlet_Preference'].min()) /\n","#    (train_df['Customer_Outlet_Preference'].max() - train_df['Customer_Outlet_Preference'].min())\n","#)\n","#valid_df['Customer_Outlet_Preference'] = (\n","#    (valid_df['Customer_Outlet_Preference'] - valid_df['Customer_Outlet_Preference'].min()) /\n","#    (valid_df['Customer_Outlet_Preference'].max() - valid_df['Customer_Outlet_Preference'].min())\n","#)\n","\n","# --- Features & Target ---\n","#features = [\n","#    'Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","#    'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","#    'Outlet_Type', 'Outlet_Age', 'Item_Sales_Frequency',\n","#    'Customer_Outlet_Preference'\n","#]\n","\n","\n","features = [\n","    'Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Item_MRP',\n","    'Item_Visibility', 'Outlet_Size', 'Outlet_Location_Type',\n","    'Outlet_Type', 'Outlet_Age',\n","    'Item_Sales_Frequency', 'Customer_Outlet_Preference',\n","    'Price_per_Weight', 'Visibility_Price_Interaction',\n","    'Outlet_Age_Type', 'Fat_Type_Interaction',\n","    'Outlet_Diversity', 'Log_MRP', 'Log_Visibility'\n","]\n","\n","\n","\n","X = train_df[features]\n","y = train_df['Item_Outlet_Sales']\n","X_test_final = valid_df[features]\n","\n","# --- Train/Validation Split ---\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)"],"metadata":{"id":"Z3mQyL2-Pevy","executionInfo":{"status":"ok","timestamp":1755328202439,"user_tz":-330,"elapsed":1130,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# --- Polynomial Regression with Lasso Tuning ---\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","rmse_scorer = make_scorer(\n","    lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n","    greater_is_better=False\n",")\n","\n","poly_lasso = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('poly', PolynomialFeatures(include_bias=False)),\n","    ('lasso', Lasso(max_iter=500))\n","])\n","\n","param_grid = {\n","    'poly__degree': [2, 3],\n","    'lasso__alpha': [0.001, 0.01, 0.05, 0.1, 0.5]\n","}\n","\n","grid = GridSearchCV(poly_lasso, param_grid, cv=kf, scoring=rmse_scorer, n_jobs=-1, verbose=1)\n","numerical_features = X_train.select_dtypes(include=[np.number]).columns\n","grid.fit(X_train[numerical_features], y_train)\n","\n","best_poly_lasso = grid.best_estimator_\n","y_pred_poly = best_poly_lasso.predict(X_val[numerical_features])\n","rmse_poly = np.sqrt(mean_squared_error(y_val, y_pred_poly))\n","cv_scores = -grid.best_score_\n","\n","print(\"\\nBest params:\", grid.best_params_)\n","print(f\"Polynomial Regression with Lasso RMSE: {rmse_poly:.2f}\")\n","print(f\"Polynomial Regression with Lasso CV RMSE: {cv_scores:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"etka5nr8clpy","executionInfo":{"status":"ok","timestamp":1755331690841,"user_tz":-330,"elapsed":104251,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"abfc1059-1f76-4a88-ccc4-e8ebaa6f1b61"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","\n","Best params: {'lasso__alpha': 0.5, 'poly__degree': 2}\n","Polynomial Regression with Lasso RMSE: 1038.14\n","Polynomial Regression with Lasso CV RMSE: 1091.63\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bSsv7XaJhY25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1ze2QK2n06_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Etuj9QmU07Ey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2qOnQvcn07Kw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Predict on full validation dataset ---\n","X_valid = valid_df[features]\n","numerical_features_valid = X_valid.select_dtypes(include=[np.number]).columns\n","y_pred_valid = best_poly_lasso.predict(X_valid[numerical_features_valid])\n","\n","# --- Save predictions to CSV ---\n","submission_poly = valid_df[['Item_Identifier', 'Outlet_Identifier']].copy()\n","submission_poly['Item_Outlet_Sales'] = y_pred_valid\n","\n","# Ensure no negative predictions\n","submission_poly['Item_Outlet_Sales'] = submission_poly['Item_Outlet_Sales'].clip(lower=0)\n","\n","submission_poly.to_csv(\"validation_predictions.csv\", index=False)\n","\n","# --- Print first few predictions ---\n","print(submission_poly.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7l3lwhQs6rsx","executionInfo":{"status":"ok","timestamp":1755331732758,"user_tz":-330,"elapsed":37,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"5f9ced31-1d7d-46a0-dcd7-89f667d6fc96"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n","0           FDW58            OUT049        1693.817586\n","1           FDW14            OUT017        1404.158092\n","2           NCN55            OUT010         539.290492\n","3           FDQ58            OUT017        2506.617873\n","4           FDY38            OUT027        6292.848042\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JoEuY4ZJ78XK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qnUuZC9u78Qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Predict on full validation dataset using numerical features from training ---\n","X_valid = valid_df[features]\n","numerical_features_train = X_train.select_dtypes(include=[np.number]).columns  # same as training\n","\n","# Predict\n","y_pred_valid = best_poly_lasso.predict(X_valid[numerical_features_train])\n","\n","# --- Save predictions to CSV ---\n","submission_poly = valid_df[['Item_Identifier', 'Outlet_Identifier']].copy()\n","submission_poly['Item_Outlet_Sales'] = y_pred_valid\n","\n","# Ensure no negative predictions\n","submission_poly['Item_Outlet_Sales'] = submission_poly['Item_Outlet_Sales'].clip(lower=0)\n","\n","# Save CSV\n","submission_poly.to_csv(\"validation_predictions_1038.csv\", index=False)\n","\n","# Print first few predictions\n","print(submission_poly.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z47OBz9b78K9","executionInfo":{"status":"ok","timestamp":1755332102905,"user_tz":-330,"elapsed":26,"user":{"displayName":"Vamshi Mohan","userId":"00739284468631637860"}},"outputId":"fb679a44-f43e-4020-b174-a8552da0be46"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n","0           FDW58            OUT049        1693.817586\n","1           FDW14            OUT017        1404.158092\n","2           NCN55            OUT010         539.290492\n","3           FDQ58            OUT017        2506.617873\n","4           FDY38            OUT027        6292.848042\n"]}]}]}